* today i learned
* 2017
** 2017-11 November
*** 2017-11-03 Friday
**** build a datalake
https://aws.amazon.com/blogs/big-data/build-a-data-lake-foundation-with-aws-glue-and-amazon-s3/
**** amazon cost calculator
http://www.ec2instances.info/?min_memory=300&min_vcpus=64&region=eu-west-1&cost_duration=daily
**** AWS calculating costs
   [[file:~/gtd/til.org::*amazon%20cost%20calculator][amazon cost calculator]]
   http://www.ec2instances.info/ - good for costing instances and comparing cpu/ram/etc
   https://calculator.s3.amazonaws.com/index.html - good for estimating costs of lots of AWS services, including the extras like EMR overhead, EBS, bandwidth, etc.
   calculator.s3.amazonaws.com
   Amazon Web Services Simple Monthly Calculator
   The AWS Simple Monthly Calculator helps customers and prospects
   estimate their monthly AWS bill more efficiently. Using this tool,
   they can add, modify and remove services from their 'bill' and it will
   recalculate their estimated monthly charges automatically. The
   calculator also shows common customer samples and their usage, such as
   Disaster Recovery and Backup or Web Application.
   gavin [2:45 PM]
   I like ec2instances.info to be able to compare things, e.g. Irish instances with at least 64 vcpus and 300GB RAM, shown as price per day: http://www.ec2instances.info/?min_memory=300&min_vcpus=64&region=eu-west-1&cost_duration=daily
*** 2017-11-09 Thursday
**** H20 statistical library
*** 2017-11-10 Friday
**** spacial athena queries
https://twitter.com/awscloud/status/925824934538104832
**** aws glue samples
https://github.com/awslabs/aws-glue-samples
**** self driving car, workbooks
https://github.com/awslabs/aws-glue-samples
**** loss models book
https://mukuba2002.files.wordpress.com/2012/03/44850471215775.pdf
**** speeding up joins by using known partioner

Speeding up joins by assigning a known partitioner

If you have to do an operation before the join that requires a shuffle, such as aggregateByKey or reduceByKey, you can prevent the shuffle by adding a hash partitioner with the same number of partitions as an explicit argument to the first operation before the join. You could make the example in the previous section even faster, by using the partitioner for the address data as an argument for the reduceByKey step, as in Example 4-4 and Figure 4-4.
Example 4-4. Known partitioner join

  def joinScoresWithAddress3(scoreRDD: RDD[(Long, Double)],
   addressRDD: RDD[(Long, String)]) : RDD[(Long, (Double, String))]= {
    // If addressRDD has a known partitioner we should use that,
    // otherwise it has a default hash parttioner, which we can reconstruct by
    // getting the number of partitions.
    val addressDataPartitioner = addressRDD.partitioner match {
      case (Some(p)) => p
      case (None) => new HashPartitioner(addressRDD.partitions.length)
    }
    val bestScoreData = scoreRDD.reduceByKey(addressDataPartitioner,
      (x, y) => if(x > y) x else y)
    bestScoreData.join(addressRDD)
  }
**** mapGroupsWithState
a way of working with structured streaming as an alternative to window functions
**** spark window functions
https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html
**** fast spark joins

https://www.snappydata.io/blog/joining-billion-rows-faster-than-apache-spark
*** 2017-11-14 Tuesday
**** querying the trip data in athena
#+BEGIN_SRC sql
SELECT * FROM "pgr"."filter_smooth_map_join_train_test" limit 10;
#+END_SRC
*** 2017-11-16 Thursday
**** Drivebot
https://github.com/mydrive/drivebot-docker
*** 2017-11-17 Friday
**** docker with conda and channels
channels are just repos for conda
miniconda has some packages preinstalled
anaconda has loads
#+BEGIN_SRC dockerfile
FROM continuumio/miniconda3:latest

LABEL maintainer "David Loughnane"

RUN conda install --yes \
    numpy \
    scipy \
    pandas \
    scikit-learn \
    matplotlib \
    cython \
    boto3

RUN conda install --yes -c conda-forge jupyterlab

WORKDIR home/

# RUN mkdir -p /root/app/
# ADD lib /root/app/lib
# ADD filter_smooth.py /root/app/

# WORKDIR /root/app/
# CMD [ "python", "filter_smooth.py" ]

#+END_SRC
****   Docker with EC2 and Zepplin

***** Install Docker
 Install Docker from here: https://docs.docker.com/docker-for-mac/install/#download-docker-for-mac
 Create a new Docker account for Docker
 EC2 instance with Docker container and Anaconda
 Start Docker on your local machine
 Create an EC2 instance
 Switch to the right profile role (e.g. pgr) and go to EC2 -> Launch Instance.
 Ensure you are in the right region (eg us-east-1, etc)
 Choose a Machine Image and specify the configurations
 Security Group: The instance should allow inbound traffic from ‘my IP’ on a certain port or port range (e.g. 8888)
 IAM role: The IAM role used on the instance should allow the instance to access other AWS services such as S3. Recommend to use the Data Scientist policy.
 Review and Launch the instance.
 Existing instances can be viewed in the Instances list on the navigation panel on the left.
 SSH into your instance following the instructions on AWS page. Example:
 $ ssh -i "your_key_pair.pem" ubuntu@ec2-34-204-107-135.compute-1.amazonaws.com
 Create a Docker container on the EC2 instance
 SSH into your instance
 sudo yum install docker on the instance
 $ sudo yum install -y docker
 Start the docker service
 $ sudo service docker start
 $ sudo usermod -a -G docker ec2-user
 Reboot the instance and login again
 Pull the Anaconda docker image from Dockerhub
 $ docker pull continuumio/anaconda3
 Create a docker container from the image and publish the container’s port to the host’s port
 $ docker run -i -t -d -p 8880:8880 continuumio/anaconda3 /bin/bash
 (copy and save the dockerID)
 Run and attach to the container
 $ docker start -a <dockerID>
 Install additional python packages if needed (e.g. boto3, pyarrow, etc)
 Install any tools if needed (e.g. nano)
 Work on jupyter notebook (follow the steps below or check the section Run jupyter notebook on EC2 instance)
 Run jupyter notebook
 # mkdir /opt/notebooks/
 # jupyter notebook --allow-root --ip='*' --no-browser --port=8880 --notebook-dir=/opt/notebooks
 (run with nohup if you want to run other commands)
 Connect to the notebook in the browser on your local machine
 http://<ec2_instance_public_ip>:<container_port>/?token=xxxxxxx
 Code in the notebook as normal. For example, read/write to S3 buckets via boto3 session client.
 Exit the docker container
 # exit will exit and kill the container
 $ docker ps -a list all the containers
 $ docker start to re-start a killed container
**** Autonomous Driving learning

http://www.carla.org/

CARLA is an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions.
*** 2017-11-22 Wednesday
**** postgres
- gis tools for geo queries
- pgpool clustering postgres servers
- aurorora
*** 2017-11-27 Monday
**** build web apps for ios and android
https://ionicframework.com/
*** 2017-11-29 Wednesday
**** broadcast hash join by default
causes timeout
persist both dataframes
** 2017-12 December
*** 2017-12-01 Friday
**** ENSIME jump to next error
  M-n
M-p
<<<<<<< variant A
>>>>>>> variant B
**** DataFrame.dropDuplicates
does what it says on the tin!
======= end
*** 2017-12-04 Monday
****
*** 2017-12-05 Tuesday
**** publish to ecs

***** create repository
https://console.aws.amazon.com/ecs/home?region=us-east-1#/repositories
***** get a command to log into ecs
this gives you a command to log into ecs
#+BEGIN_SRC bash
ecs aws ecr get-login --region us-east-1
#+END_SRC

***** build a docker image
#+BEGIN_SRC bash
docker build -t docker-ecs .
#+END_SRC

***** tag docker image
you'll need the repo url = here it is 047143111768.dkr.ecr.us-east-1.amazon.com/docker-ecs
#+BEGIN_SRC bash
docker tag docker-ecs:latest 047143111768.dkr.ecr.us-east-1.amazon.com/docker-ecs/docker-ecs:latest

#+END_SRC

***** push to the repoistory
#+BEGIN_SRC bash
docker push 047143111768.dkr.ecr.us-east-1.amazon.com/docker-ecss:latest

#+END_SRC** 2017-12 December
        #+BEGIN_SRC
        FARGATE AWS for docker containers

        #+END_SRC
*** sshuttle
used for VPNs
*** sagemaker
made of 3 independent services?
**** managed notebook servers
**** libraries for training models
**** model hosting
*** lift from features
 - hard corning small feature lift
*** 2017-12-06 Wednesday
**** Mydrive mission statement
to provide customers with captibitly to have insight which add
demonstrabl business value
go to company to support generali digital transformation
help at worst and drive at best come up with ideas to change and
digitize generali s digitization


dont expect answers from board
make this into a world class datascience company
predictive analytics of driving behaviour
**** openstreet map on s3
available on amazon
can build athena table on top
*** 2017-12-11 Monday
**** sshuttle
#+BEGIN_SRC bash
sshuttle --dns -r ec2-52-213-141-240.eu-west-1.compute.amazonaws.com 10.0.0.0/16`
#+END_SRC

 in my .ssh/config I’ve got:
#+BEGIN_SRC
Host *.compute.amazonaws.com *.compute.internal
ForwardAgent yes
#+END_SRC
**** KVal Cross Validation
split into k-folds
#+BEGIN_EXAMPLE
(k=)10 test and train sets,
then take an average of ASE score
#+END_EXAMPLE
**** hard brake

**** why put a filtering on speed
filtering gave more reasonable speed on mobile data
was originally implemented to create better data on maps
**** progressive
ase 1500 with all features
**** hardbrake
based on gps not obs data

**** here data
can you learn how to mapmatch on progressive side?
*** 2017-12-12 Tuesday
**** UBI
usage based insurance
**** group de assuranceces mutulle
group ama
*** 2017-12-15 Friday
**** turicreate
apple system for ml
*** 2017-12-18 Monday
**** cocoapods for podfiles
#+BEGIN_SRC bash
sudo gem install cocoapods
#+END_SRC
then from the directory
#+BEGIN_SRC bash
pod install

#+END_SRC** mydrive germany market
**** generali
brokers
dedicated sales org dvag
self employed fin advice network
**** archenouchima
mostly sell to young drivers
**** genetal
*** 2017-12-20 Wednesday
**** how to capture all s3 from mydrive repo
#+BEGIN_SRC bash
find . -type f -print0  |  xargs -0 awk 'match($0,/s3:[^\"'\'']*/) {print substr($0,RSTART,RLENGTH)}' > ~/dev/mydrive/alls3fromcode.txt

#+END_SRC
**** data generator for kinesis
https://awslabs.github.io/amazon-kinesis-data-generator/web/help.html
