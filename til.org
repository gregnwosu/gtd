* today i learned
* 2017
** 2017-11 November
*** 2017-11-03 Friday
**** build a datalake
https://aws.amazon.com/blogs/big-data/build-a-data-lake-foundation-with-aws-glue-and-amazon-s3/
**** amazon cost calculator
http://www.ec2instances.info/?min_memory=300&min_vcpus=64&region=eu-west-1&cost_duration=daily
**** AWS calculating costs
   [[file:~/gtd/til.org::*amazon%20cost%20calculator][amazon cost calculator]]
   http://www.ec2instances.info/ - good for costing instances and comparing cpu/ram/etc
   https://calculator.s3.amazonaws.com/index.html - good for estimating costs of lots of AWS services, including the extras like EMR overhead, EBS, bandwidth, etc.
   calculator.s3.amazonaws.com
   Amazon Web Services Simple Monthly Calculator
   The AWS Simple Monthly Calculator helps customers and prospects
   estimate their monthly AWS bill more efficiently. Using this tool,
   they can add, modify and remove services from their 'bill' and it will
   recalculate their estimated monthly charges automatically. The
   calculator also shows common customer samples and their usage, such as
   Disaster Recovery and Backup or Web Application.
   gavin [2:45 PM]
   I like ec2instances.info to be able to compare things, e.g. Irish instances with at least 64 vcpus and 300GB RAM, shown as price per day: http://www.ec2instances.info/?min_memory=300&min_vcpus=64&region=eu-west-1&cost_duration=daily
*** 2017-11-09 Thursday
**** H20 statistical library
*** 2017-11-10 Friday
**** spacial athena queries
https://twitter.com/awscloud/status/925824934538104832
**** aws glue samples
https://github.com/awslabs/aws-glue-samples
**** self driving car, workbooks
https://github.com/awslabs/aws-glue-samples
**** loss models book
https://mukuba2002.files.wordpress.com/2012/03/44850471215775.pdf
**** speeding up joins by using known partioner

Speeding up joins by assigning a known partitioner

If you have to do an operation before the join that requires a shuffle, such as aggregateByKey or reduceByKey, you can prevent the shuffle by adding a hash partitioner with the same number of partitions as an explicit argument to the first operation before the join. You could make the example in the previous section even faster, by using the partitioner for the address data as an argument for the reduceByKey step, as in Example 4-4 and Figure 4-4.
Example 4-4. Known partitioner join

  def joinScoresWithAddress3(scoreRDD: RDD[(Long, Double)],
   addressRDD: RDD[(Long, String)]) : RDD[(Long, (Double, String))]= {
    // If addressRDD has a known partitioner we should use that,
    // otherwise it has a default hash parttioner, which we can reconstruct by
    // getting the number of partitions.
    val addressDataPartitioner = addressRDD.partitioner match {
      case (Some(p)) => p
      case (None) => new HashPartitioner(addressRDD.partitions.length)
    }
    val bestScoreData = scoreRDD.reduceByKey(addressDataPartitioner,
      (x, y) => if(x > y) x else y)
    bestScoreData.join(addressRDD)
  }
**** mapGroupsWithState
a way of working with structured streaming as an alternative to window functions
**** spark window functions
https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html
**** fast spark joins

https://www.snappydata.io/blog/joining-billion-rows-faster-than-apache-spark
*** 2017-11-14 Tuesday
**** querying the trip data in athena
#+BEGIN_SRC sql
SELECT * FROM "pgr"."filter_smooth_map_join_train_test" limit 10;
#+END_SRC
*** 2017-11-16 Thursday
**** Drivebot
https://github.com/mydrive/drivebot-docker
*** 2017-11-17 Friday
**** docker with conda and channels
channels are just repos for conda
miniconda has some packages preinstalled
anaconda has loads
#+BEGIN_SRC dockerfile
FROM continuumio/miniconda3:latest

LABEL maintainer "David Loughnane"

RUN conda install --yes \
    numpy \
    scipy \
    pandas \
    scikit-learn \
    matplotlib \
    cython \
    boto3

RUN conda install --yes -c conda-forge jupyterlab

WORKDIR home/

# RUN mkdir -p /root/app/
# ADD lib /root/app/lib
# ADD filter_smooth.py /root/app/

# WORKDIR /root/app/
# CMD [ "python", "filter_smooth.py" ]

#+END_SRC
****   Docker with EC2 and Zepplin

***** Install Docker
 Install Docker from here: https://docs.docker.com/docker-for-mac/install/#download-docker-for-mac
 Create a new Docker account for Docker
 EC2 instance with Docker container and Anaconda
 Start Docker on your local machine
 Create an EC2 instance
 Switch to the right profile role (e.g. pgr) and go to EC2 -> Launch Instance.
 Ensure you are in the right region (eg us-east-1, etc)
 Choose a Machine Image and specify the configurations
 Security Group: The instance should allow inbound traffic from ‘my IP’ on a certain port or port range (e.g. 8888)
 IAM role: The IAM role used on the instance should allow the instance to access other AWS services such as S3. Recommend to use the Data Scientist policy.
 Review and Launch the instance.
 Existing instances can be viewed in the Instances list on the navigation panel on the left.
 SSH into your instance following the instructions on AWS page. Example:
 $ ssh -i "your_key_pair.pem" ubuntu@ec2-34-204-107-135.compute-1.amazonaws.com
 Create a Docker container on the EC2 instance
 SSH into your instance
 sudo yum install docker on the instance
 $ sudo yum install -y docker
 Start the docker service
 $ sudo service docker start
 $ sudo usermod -a -G docker ec2-user
 Reboot the instance and login again
 Pull the Anaconda docker image from Dockerhub
 $ docker pull continuumio/anaconda3
 Create a docker container from the image and publish the container’s port to the host’s port
 $ docker run -i -t -d -p 8880:8880 continuumio/anaconda3 /bin/bash
 (copy and save the dockerID)
 Run and attach to the container
 $ docker start -a <dockerID>
 Install additional python packages if needed (e.g. boto3, pyarrow, etc)
 Install any tools if needed (e.g. nano)
 Work on jupyter notebook (follow the steps below or check the section Run jupyter notebook on EC2 instance)
 Run jupyter notebook
 # mkdir /opt/notebooks/
 # jupyter notebook --allow-root --ip='*' --no-browser --port=8880 --notebook-dir=/opt/notebooks
 (run with nohup if you want to run other commands)
 Connect to the notebook in the browser on your local machine
 http://<ec2_instance_public_ip>:<container_port>/?token=xxxxxxx
 Code in the notebook as normal. For example, read/write to S3 buckets via boto3 session client.
 Exit the docker container
 # exit will exit and kill the container
 $ docker ps -a list all the containers
 $ docker start to re-start a killed container
**** Autonomous Driving learning

http://www.carla.org/

CARLA is an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions.
*** 2017-11-22 Wednesday
**** postgres
- gis tools for geo queries
- pgpool clustering postgres servers
- aurorora
*** 2017-11-27 Monday
**** build web apps for ios and android
https://ionicframework.com/
*** 2017-11-29 Wednesday
**** broadcast hash join by default
causes timeout
persist both dataframes
** 2017-12 December
*** 2017-12-01 Friday
**** ENSIME jump to next error
  M-n
M-p
